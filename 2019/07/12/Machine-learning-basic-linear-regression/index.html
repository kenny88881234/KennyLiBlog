<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>筆記 - 機器學習 基礎與線性回歸 | Kenny's Blog</title><meta name="description" content="筆記 - 機器學習 基礎與線性回歸"><meta name="keywords" content="筆記,機器學習,線性回歸"><meta name="author" content="Kenny Li"><meta name="copyright" content="Kenny Li"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="https://kennyliblog.nctu.me/2019/07/12/Machine-learning-basic-linear-regression/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="筆記 - 機器學習 基礎與線性回歸"><meta name="twitter:description" content="筆記 - 機器學習 基礎與線性回歸"><meta name="twitter:image" content="/img/note.jpg"><meta property="og:type" content="article"><meta property="og:title" content="筆記 - 機器學習 基礎與線性回歸"><meta property="og:url" content="https://kennyliblog.nctu.me/2019/07/12/Machine-learning-basic-linear-regression/"><meta property="og:site_name" content="Kenny's Blog"><meta property="og:description" content="筆記 - 機器學習 基礎與線性回歸"><meta property="og:image" content="/img/note.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="筆記 - Octave 基礎" href="https://kennyliblog.nctu.me/2019/07/12/Octave-basic/"><link rel="next" title="問題 - BeautifulSoup 解析後無法存入 MySQL" href="https://kennyliblog.nctu.me/2019/07/01/BeautifulSoup-save-to-MySQL-problem/"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"cookieDomain":"https://KennyLi.me/","msgToTraditionalChinese":"简","msgToSimplifiedChinese":"繁"},
  highlight_copy: 'true',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Bookmark',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#筆記-機器學習-基礎與線性回歸"><span class="toc-number">1.</span> <span class="toc-text">[筆記] 機器學習 基礎與線性回歸</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#定義"><span class="toc-number">1.0.1.</span> <span class="toc-text">定義 :</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Octave-for-Microsoft-Windows"><span class="toc-number">1.0.2.</span> <span class="toc-text">Octave for Microsoft Windows</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Supervised-learing-監督式學習"><span class="toc-number">1.0.3.</span> <span class="toc-text">Supervised learing ( 監督式學習 )</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Unsupervised-learing-非監督式學習"><span class="toc-number">1.0.4.</span> <span class="toc-text">Unsupervised learing ( 非監督式學習 )</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#線性回歸算法-Linear-Regression"><span class="toc-number">1.0.5.</span> <span class="toc-text">線性回歸算法 ( Linear Regression )</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#代價函數-Cost-Function"><span class="toc-number">1.0.5.1.</span> <span class="toc-text">代價函數 ( Cost Function )</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用梯度下降-Gradient-descent-將函數-J-最小化"><span class="toc-number">1.0.5.2.</span> <span class="toc-text">使用梯度下降 ( Gradient descent ) 將函數 J 最小化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#結合梯度下降與代價函數"><span class="toc-number">1.0.5.3.</span> <span class="toc-text">結合梯度下降與代價函數</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#多特徵線性回歸-Linear-Regression-with-multiple-variables"><span class="toc-number">1.0.5.4.</span> <span class="toc-text">多特徵線性回歸 ( Linear Regression with multiple variables)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用梯度下降解多特徵線性回歸"><span class="toc-number">1.0.5.5.</span> <span class="toc-text">使用梯度下降解多特徵線性回歸</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#特徵縮放-Feature-Scaling-與均值歸一化-Mean-Normalization"><span class="toc-number">1.0.5.6.</span> <span class="toc-text">特徵縮放 ( Feature Scaling ) 與均值歸一化 ( Mean Normalization )</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#多項式回歸-Polynomial-Regression"><span class="toc-number">1.0.5.7.</span> <span class="toc-text">多項式回歸 ( Polynomial Regression )</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#正規方程-Normal-Equation"><span class="toc-number">1.0.5.8.</span> <span class="toc-text">正規方程 ( Normal Equation )</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#梯度下降-vs-正規方程"><span class="toc-number">1.0.5.9.</span> <span class="toc-text">梯度下降 vs 正規方程</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#tags-筆記-機器學習-線性回歸"><span class="toc-number">1.0.5.9.0.1.</span> <span class="toc-text">tags: 筆記 機器學習 線性回歸</span></a></li></ol></li></ol></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/note.jpg)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">Kenny's Blog</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">筆記 - 機器學習 基礎與線性回歸</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2019-07-12<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2019-07-25</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/筆記/">筆記</a></span></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="筆記-機器學習-基礎與線性回歸"><a href="#筆記-機器學習-基礎與線性回歸" class="headerlink" title="[筆記] 機器學習 基礎與線性回歸"></a>[筆記] 機器學習 基礎與線性回歸</h1><h3 id="定義"><a href="#定義" class="headerlink" title="定義 :"></a>定義 :</h3><blockquote>
<p> A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>
</blockquote>
<p><img src="https://i.imgur.com/pBIYQVb.png" alt></p>
<ul>
<li>使用訓練資料產出一個函式 <code>h</code>，再用 <code>h</code> 去判斷 input <code>x</code> 產出 output <code>y</code></li>
</ul>
<h3 id="Octave-for-Microsoft-Windows"><a href="#Octave-for-Microsoft-Windows" class="headerlink" title="Octave for Microsoft Windows"></a>Octave for Microsoft Windows</h3><ul>
<li>主要用於數值分析的軟體，在初學機器學習很好用</li>
<li>下載地址 : <a href="http://wiki.octave.org/Octave_for_Microsoft_Windows" target="_blank" rel="noopener">Octave 官方網站</a></li>
</ul>
<p><img src="https://i.imgur.com/DOyKeIN.png" alt></p>
<ul>
<li>下載地址</li>
</ul>
<p><img src="https://i.imgur.com/Dsv02Tn.png" alt></p>
<ul>
<li>下載 installer 後不斷 <code>next</code> 就能安裝完成了</li>
</ul>
<p>P.S.下載任何版本都可以，但不要下載 <code>Octave 4.0.0</code>，此版本有重大的 bug</p>
<h3 id="Supervised-learing-監督式學習"><a href="#Supervised-learing-監督式學習" class="headerlink" title="Supervised learing ( 監督式學習 )"></a>Supervised learing ( 監督式學習 )</h3><ul>
<li>定義 : 機器去學習事先標記過的訓練範例 （ 輸入和預期輸出 ） 後，去預測這個函數對任何可能出現的輸入的輸出</li>
<li><p>函數的輸出可以分為兩類 :</p>
<ul>
<li>回歸分析 ( Regression ) : 輸出連續的值，例如 : 房價</li>
<li>分類 ( Classification ) : 輸出一個分類的標籤，例如 : 有或沒有</li>
</ul>
</li>
</ul>
<h3 id="Unsupervised-learing-非監督式學習"><a href="#Unsupervised-learing-非監督式學習" class="headerlink" title="Unsupervised learing ( 非監督式學習 )"></a>Unsupervised learing ( 非監督式學習 )</h3><ul>
<li>定義 : 沒有給定事先標記過的訓練範例，自動對輸入的資料進行分類或分群</li>
<li><p>常用於分群，有兩類應用 :</p>
<ul>
<li>聚類 ( Clustering ) : 將資料集中的樣本劃分為若干個通常是不相交的子集，例如 : 分類成不同類別的新聞</li>
<li>非聚類 ( Non-clustering ) : 例如 : 雞尾酒會演算法，從帶有噪音的資料中找到有效資料，可用於語音辨識</li>
</ul>
</li>
</ul>
<h3 id="線性回歸算法-Linear-Regression"><a href="#線性回歸算法-Linear-Regression" class="headerlink" title="線性回歸算法 ( Linear Regression )"></a>線性回歸算法 ( Linear Regression )</h3><ul>
<li><code>hθ(x) = θ₀ + θ₁x₁</code> : 線性回歸算式</li>
<li><code>m</code> : 資料量</li>
<li><code>x⁽ⁱ⁾</code> : i 代表第 i 筆資料</li>
</ul>
<h4 id="代價函數-Cost-Function"><a href="#代價函數-Cost-Function" class="headerlink" title="代價函數 ( Cost Function )"></a>代價函數 ( Cost Function )</h4><p><img src="https://i.imgur.com/2UNCKfj.png" alt></p>
<ul>
<li>算出代價函數</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">function J = costFunctionJ(X, y, theta)</span><br><span class="line"></span><br><span class="line">m = length(y);</span><br><span class="line">J = 0</span><br><span class="line"></span><br><span class="line">predictions = X * theta;</span><br><span class="line">sqrErrors = (predictions - y).^2;</span><br><span class="line"></span><br><span class="line">J = 1 / (2 * m) * sum(sqrErrors);</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<ul>
<li>在 Octave 上的代價函數函式</li>
</ul>
<p><img src="https://i.imgur.com/ryFygNy.png" alt></p>
<ul>
<li>找出代價函數的最小值，來找出 θ₀、θ₁</li>
</ul>
<h4 id="使用梯度下降-Gradient-descent-將函數-J-最小化"><a href="#使用梯度下降-Gradient-descent-將函數-J-最小化" class="headerlink" title="使用梯度下降 ( Gradient descent ) 將函數 J 最小化"></a>使用梯度下降 ( Gradient descent ) 將函數 J 最小化</h4><ol>
<li>初始化 θ₀、θ₁ ( θ₀=0, θ₁=0 也可以是其他值)</li>
<li>不斷改變 θ₀、θ₁ 直到找到最小值，或許是局部最小值</li>
</ol>
<p><img src="https://i.imgur.com/lDD0Y7J.png" alt></p>
<ul>
<li>梯度下降公式，不斷運算直到收斂，θ₀、θ₁ 必須同時更新</li>
<li>α 後的公式其實就是導數 ( 一點上的切線斜率 )</li>
<li>α 是 learning rate</li>
</ul>
<p><img src="https://i.imgur.com/PLkKjd4.png" alt></p>
<ul>
<li>正確的算法</li>
</ul>
<p><img src="https://i.imgur.com/CmwvFgc.png" alt></p>
<ul>
<li>錯誤的算法，沒有同步更新</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)</span><br><span class="line"></span><br><span class="line">m = length(y);</span><br><span class="line">J_history = zeros(num_iters, 1);</span><br><span class="line"></span><br><span class="line">for iter = 1:num_iters</span><br><span class="line"></span><br><span class="line">    delta = 1 / m * (X&apos; * X * theta - X&apos; * y);</span><br><span class="line">    theta = theta - alpha .* delta;</span><br><span class="line"></span><br><span class="line">    J_history(iter) = computeCost(X, y, theta);</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<ul>
<li>在 Octave 上的梯度下降函式</li>
</ul>
<p><strong>Learning Rate α</strong></p>
<ul>
<li>α 是 learning rate，控制以多大幅度更新 θ₀、θ₁</li>
<li>決定 α 最好的方式是隨著絕對值的導數更新，絕對值的導數越大，α 越大</li>
<li>α 可以從 0.001 開始 ( 每次 3 倍 )</li>
<li>α 太小 : 收斂會很緩慢</li>
<li>α 太大 : 可能造成代價函數無法下降，甚至無法收斂</li>
</ul>
<h4 id="結合梯度下降與代價函數"><a href="#結合梯度下降與代價函數" class="headerlink" title="結合梯度下降與代價函數"></a>結合梯度下降與代價函數</h4><p><img src="https://i.imgur.com/kR0TFuh.png" alt></p>
<ul>
<li>將代價函數帶入梯度下降公式</li>
<li>以<strong>所有樣本</strong>帶入梯度下降公式不斷尋找 θ₀、θ₁，在機器學習裡稱作批量梯度下降 ( batch gradient descent )</li>
</ul>
<h4 id="多特徵線性回歸-Linear-Regression-with-multiple-variables"><a href="#多特徵線性回歸-Linear-Regression-with-multiple-variables" class="headerlink" title="多特徵線性回歸 ( Linear Regression with multiple variables)"></a>多特徵線性回歸 ( Linear Regression with multiple variables)</h4><ul>
<li><code>hθ(x) = θ₀x₀ + θ₁x₁ + θ₂x₂ + ... + θₙxₙ</code> : 多特徵線性回歸算式，x₀ = 1</li>
<li><code>n</code> : 特徵量</li>
</ul>
<p><img src="https://i.imgur.com/pOI4Lqt.png" alt></p>
<h4 id="使用梯度下降解多特徵線性回歸"><a href="#使用梯度下降解多特徵線性回歸" class="headerlink" title="使用梯度下降解多特徵線性回歸"></a>使用梯度下降解多特徵線性回歸</h4><p><img src="https://i.imgur.com/4QvQlBf.png" alt></p>
<ul>
<li>相較於一元線性回歸，只是多出最後的 xⱼ</li>
</ul>
<p><img src="https://i.imgur.com/ATD0b1b.png" alt></p>
<ul>
<li>拆開後</li>
</ul>
<h4 id="特徵縮放-Feature-Scaling-與均值歸一化-Mean-Normalization"><a href="#特徵縮放-Feature-Scaling-與均值歸一化-Mean-Normalization" class="headerlink" title="特徵縮放 ( Feature Scaling ) 與均值歸一化 ( Mean Normalization )"></a>特徵縮放 ( Feature Scaling ) 與均值歸一化 ( Mean Normalization )</h4><ul>
<li>目的 : 加快梯度下降，因為特徵值範圍相差過大會導致梯度下降緩慢</li>
</ul>
<p><img src="https://i.imgur.com/HTHEYLJ.png" alt></p>
<ul>
<li><code>sᵢ</code> : 特徵縮放，通常使用數值範圍</li>
<li><code>μᵢ</code> : 均值歸一化，通常使用數值的平均</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">function [X_norm, mu, sigma] = featureNormalize(X)</span><br><span class="line"></span><br><span class="line">X_norm = X;</span><br><span class="line">mu = zeros(1, size(X, 2));</span><br><span class="line">sigma = zeros(1, size(X, 2));      </span><br><span class="line"></span><br><span class="line">mu = mean(X);</span><br><span class="line">sigma = std(X);</span><br><span class="line"></span><br><span class="line">for i = 1:size(X, 2)</span><br><span class="line"></span><br><span class="line">    X_mu = X(:, i) - mu(i);</span><br><span class="line">    X_norm(:, i) = X_mu ./ sigma(i);</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<ul>
<li>在 Octave 上的特徵縮放與均值歸一化函式</li>
</ul>
<h4 id="多項式回歸-Polynomial-Regression"><a href="#多項式回歸-Polynomial-Regression" class="headerlink" title="多項式回歸 ( Polynomial Regression )"></a>多項式回歸 ( Polynomial Regression )</h4><ul>
<li>我們可以結合多種有關的特徵，產生一個新的特徵，例如 : 房子長、寬結合成房子面積</li>
<li>假如線性的 ( 直線 ) 函式無法很好的符合數據，我們也可以使用二次、三次或平方根函式 ( 或其他任何的形式 )</li>
</ul>
<h4 id="正規方程-Normal-Equation"><a href="#正規方程-Normal-Equation" class="headerlink" title="正規方程 ( Normal Equation )"></a>正規方程 ( Normal Equation )</h4><p>X = 各特徵值<br>y = 各結果</p>
<ul>
<li>算式 : <code>(XᵀX)⁻¹Xᵀy</code></li>
<li>Octave : <code>pinv(X&#39;*X)*X&#39;*y</code></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">function [theta] = normalEqn(X, y)</span><br><span class="line"></span><br><span class="line">theta = zeros(size(X, 2), 1);</span><br><span class="line"></span><br><span class="line">theta = pinv(X&apos; * X) * X&apos; * y</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<ul>
<li>在 Octave 上的正規方程函式</li>
</ul>
<p>在 Octave 裡我們通常用 <code>pinv</code> 而不是 <code>inv</code>，因為使用 <code>pinv</code> 就算 <code>XᵀX</code> 為不可逆，還是會給予 θ 的值</p>
<ul>
<li><p><code>XᵀX</code> 不可逆的原因 :</p>
<ul>
<li>多餘且無關的特徵值</li>
<li>特徵值過多 ( m&lt;=n )，刪除一些或正規化</li>
</ul>
</li>
</ul>
<h4 id="梯度下降-vs-正規方程"><a href="#梯度下降-vs-正規方程" class="headerlink" title="梯度下降 vs 正規方程"></a>梯度下降 vs 正規方程</h4><ul>
<li><p>梯度下降</p>
<ul>
<li><p>優點 : </p>
<ul>
<li>特徵量大時，可以正常運作</li>
<li>O(kn²)</li>
</ul>
</li>
<li><p>缺點 : </p>
<ul>
<li>需要選擇 α</li>
<li>需要不斷迭代</li>
</ul>
</li>
</ul>
</li>
<li><p>正規方程</p>
<ul>
<li><p>優點 : </p>
<ul>
<li>不需要選擇 α</li>
<li>不用迭代</li>
</ul>
</li>
<li><p>缺點 : </p>
<ul>
<li>需運算 (XᵀX)⁻¹，所以當特徵量大時，會耗費很多運算時間 ( n &gt; 10000 )</li>
<li>O(n³)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h6 id="tags-筆記-機器學習-線性回歸"><a href="#tags-筆記-機器學習-線性回歸" class="headerlink" title="tags: 筆記 機器學習 線性回歸"></a>tags: <code>筆記</code> <code>機器學習</code> <code>線性回歸</code></h6></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Kenny Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://kennyliblog.nctu.me/2019/07/12/Machine-learning-basic-linear-regression/">https://kennyliblog.nctu.me/2019/07/12/Machine-learning-basic-linear-regression/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/筆記/">筆記    </a><a class="post-meta__tags" href="/tags/機器學習/">機器學習    </a><a class="post-meta__tags" href="/tags/線性回歸/">線性回歸    </a></div><div class="post_share"><div class="social-share" data-image="/img/note.jpg" data-sites="facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-left"><a href="/2019/07/12/Octave-basic/"><img class="prev_cover lozad" data-src="/img/note.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Previous Post</div><div class="prev_info"><span>筆記 - Octave 基礎</span></div></a></div><div class="next-post pull-right"><a href="/2019/07/01/BeautifulSoup-save-to-MySQL-problem/"><img class="next_cover lozad" data-src="/img/problem.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>問題 - BeautifulSoup 解析後無法存入 MySQL</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/07/26/Machine-learning-neural-network-representation/" title="筆記 - 機器學習 神經網路 ( Neural Network ) - 表層結構"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 神經網路 ( Neural Network ) - 表層結構</div></a></div><div class="relatedPosts_item"><a href="/2019/07/19/Machine-learning-logistic-regression/" title="筆記 - 機器學習 邏輯回歸"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 邏輯回歸</div></a></div><div class="relatedPosts_item"><a href="/2019/07/12/Octave-basic/" title="筆記 - Octave 基礎"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - Octave 基礎</div></a></div><div class="relatedPosts_item"><a href="/2019/07/24/Machine-learning-overfitting-problem/" title="筆記 - 機器學習 過度擬合問題 ( overfitting )"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 過度擬合問題 ( overfitting )</div></a></div><div class="relatedPosts_item"><a href="/2019/08/08/Machine-learning-advice-for-model/" title="筆記 - 機器學習 有效的改善模型"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 有效的改善模型</div></a></div><div class="relatedPosts_item"><a href="/2019/08/09/Machine-learning-system-design/" title="筆記 - 機器學習 系統的設計"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 系統的設計</div></a></div></div><div class="clear_both"></div></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 By Kenny Li</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="Read Mode"> </i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion">繁</a><i class="fa fa-moon-o nightshift" id="nightshift" title="Dark Mode"></i></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/baidupush.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script></body></html>