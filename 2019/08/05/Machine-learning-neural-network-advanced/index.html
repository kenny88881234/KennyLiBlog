<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>筆記 - 機器學習 神經網路 - 進階學習 | Kenny's Blog</title><meta name="description" content="筆記 - 機器學習 神經網路 - 進階學習"><meta name="keywords" content="筆記,機器學習,神經網路"><meta name="author" content="Kenny Li"><meta name="copyright" content="Kenny Li"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="https://kennyliblog.nctu.me/2019/08/05/Machine-learning-neural-network-advanced/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="筆記 - 機器學習 神經網路 - 進階學習"><meta name="twitter:description" content="筆記 - 機器學習 神經網路 - 進階學習"><meta name="twitter:image" content="/img/note.jpg"><meta property="og:type" content="article"><meta property="og:title" content="筆記 - 機器學習 神經網路 - 進階學習"><meta property="og:url" content="https://kennyliblog.nctu.me/2019/08/05/Machine-learning-neural-network-advanced/"><meta property="og:site_name" content="Kenny's Blog"><meta property="og:description" content="筆記 - 機器學習 神經網路 - 進階學習"><meta property="og:image" content="/img/note.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="next" title="筆記 - 機器學習 神經網路 ( Neural Network ) - 表層結構" href="https://kennyliblog.nctu.me/2019/07/26/Machine-learning-neural-network-representation/"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"cookieDomain":"https://KennyLi.me/","msgToTraditionalChinese":"简","msgToSimplifiedChinese":"繁"},
  highlight_copy: 'true',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Bookmark',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#筆記-機器學習-神經網路-進階學習"><span class="toc-number">1.</span> <span class="toc-text">[筆記] 機器學習 神經網路 - 進階學習</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#代價函數與反向傳播算法-Backpropagation"><span class="toc-number">1.0.1.</span> <span class="toc-text">代價函數與反向傳播算法 ( Backpropagation )</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#神經網路的代價函數"><span class="toc-number">1.0.1.1.</span> <span class="toc-text">神經網路的代價函數</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#反向傳播算法-Backpropagation"><span class="toc-number">1.0.1.2.</span> <span class="toc-text">反向傳播算法 ( Backpropagation )</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#實作反向傳播算法"><span class="toc-number">1.0.2.</span> <span class="toc-text">實作反向傳播算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#矩陣展開成向量與還原"><span class="toc-number">1.0.2.1.</span> <span class="toc-text">矩陣展開成向量與還原</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#應用"><span class="toc-number">1.0.2.1.1.</span> <span class="toc-text">應用</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#梯度驗證-Gradient-Checking"><span class="toc-number">1.0.2.2.</span> <span class="toc-text">梯度驗證 ( Gradient Checking )</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#隨機初始化-Random-Initialization"><span class="toc-number">1.0.2.3.</span> <span class="toc-text">隨機初始化 ( Random Initialization )</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#總結"><span class="toc-number">1.0.2.4.</span> <span class="toc-text">總結</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#tags-筆記-機器學習-神經網路"><span class="toc-number">1.0.2.4.0.1.</span> <span class="toc-text">tags: 筆記 機器學習 神經網路</span></a></li></ol></li></ol></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/note.jpg)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">Kenny's Blog</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">筆記 - 機器學習 神經網路 - 進階學習</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2019-08-05<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2019-08-05</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/筆記/">筆記</a></span></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="筆記-機器學習-神經網路-進階學習"><a href="#筆記-機器學習-神經網路-進階學習" class="headerlink" title="[筆記] 機器學習 神經網路 - 進階學習"></a>[筆記] 機器學習 神經網路 - 進階學習</h1><h3 id="代價函數與反向傳播算法-Backpropagation"><a href="#代價函數與反向傳播算法-Backpropagation" class="headerlink" title="代價函數與反向傳播算法 ( Backpropagation )"></a>代價函數與反向傳播算法 ( Backpropagation )</h3><p>符號定義 :</p>
<ul>
<li><code>L</code> : 神經網路中擁有的層數</li>
<li><code>sₗ</code> : 第 l 層中的單元個數</li>
<li><code>K</code> : 輸出的單元 ( 分類 ) 個數</li>
</ul>
<h4 id="神經網路的代價函數"><a href="#神經網路的代價函數" class="headerlink" title="神經網路的代價函數"></a>神經網路的代價函數</h4><p><img src="https://i.imgur.com/Qzk5RD8.png" alt></p>
<ul>
<li><p>神經網路中的代價函數</p>
<ul>
<li>過往的正規化就是計算輸出層中的每個單元</li>
<li>而在神經網路則是機算整個網路中的所有單元</li>
<li>此 i ( 後者 ) 並非訓練資料的 i ( 前者 )</li>
</ul>
</li>
</ul>
<h4 id="反向傳播算法-Backpropagation"><a href="#反向傳播算法-Backpropagation" class="headerlink" title="反向傳播算法 ( Backpropagation )"></a>反向傳播算法 ( Backpropagation )</h4><ul>
<li><p>是一種訓練神經網路的常見方法，與梯度下降結合使用，目的是找到一組能最大限度地減小誤差的權重</p>
</li>
<li><p>演算法 :</p>
</li>
</ul>
<p>假設有訓練資料 <code>{(x⁽¹⁾, y⁽¹⁾) ... (x⁽ᵐ⁾, y⁽ᵐ⁾)}</code></p>
<p>設定 <code>Δᵢⱼ⁽ˡ⁾ := 0 ( for all l, i, j )</code></p>
<p><code>For i = 1 to m :</code></p>
<ol>
<li><p>設定 <code>a⁽¹⁾ := x⁽ⁱ⁾</code></p>
</li>
<li><p>計算 <code>a⁽ˡ⁾ ( for l = 2, 3, ..., L )</code></p>
</li>
<li><p>使用 <code>y⁽ⁱ⁾</code> 計算 <code>δ⁽ᴸ⁾ = a⁽ᴸ⁾ - y⁽ⁱ⁾</code>，L 代表 output layer</p>
</li>
<li><p>使用 <code>δ⁽ˡ⁾ = ((Θ⁽ˡ⁾)ᵀδ⁽ˡ⁺¹⁾) .* a⁽ˡ⁾ .* (1 - a⁽ˡ⁾)</code> 計算 <code>δ⁽ᴸ⁻¹⁾, δ⁽ᴸ⁻²⁾, ..., δ⁽²⁾</code></p>
<ul>
<li>激勵函數的導函數 : <code>g&#39;(z⁽ˡ⁾) = a⁽ˡ⁾ .* (1 - a⁽ˡ⁾)</code></li>
</ul>
</li>
<li><p><code>Δᵢⱼ⁽ˡ⁾ := Δᵢⱼ⁽ˡ⁾ + aⱼ⁽ˡ⁾δᵢ⁽ˡ⁺¹⁾</code>，向量化 : <code>Δ⁽ˡ⁾ := Δ⁽ˡ⁾ + δ⁽ˡ⁺¹⁾(a⁽ˡ⁾)ᵀ</code></p>
<ul>
<li><code>Dᵢⱼ⁽ˡ⁾ := (1 / m)(Δᵢⱼ⁽ˡ⁾ + λΘᵢⱼ⁽ˡ⁾)</code>，if j ≠ 0</li>
<li><code>Dᵢⱼ⁽ˡ⁾ := (1 / m) Δᵢⱼ⁽ˡ⁾</code>，if j = 0</li>
<li><img src="https://i.imgur.com/poAQcSC.png" alt></li>
</ul>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">function [J grad] = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda)</span><br><span class="line"></span><br><span class="line">Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), hidden_layer_size, (input_layer_size + 1));</span><br><span class="line"></span><br><span class="line">Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), num_labels, (hidden_layer_size + 1));</span><br><span class="line"></span><br><span class="line">m = size(X, 1);</span><br><span class="line">J = 0;</span><br><span class="line">Theta1_grad = zeros(size(Theta1));</span><br><span class="line">Theta2_grad = zeros(size(Theta2));</span><br><span class="line"></span><br><span class="line">% 前向傳播算法</span><br><span class="line"></span><br><span class="line">a1 = [ones(m, 1) X];</span><br><span class="line"></span><br><span class="line">z2 = a1 * Theta1&apos;;</span><br><span class="line">a2 = sigmoid(z2);</span><br><span class="line">a2 = [ones(size(a2, 1), 1) a2];</span><br><span class="line"></span><br><span class="line">z3 = a2 * Theta2&apos;;</span><br><span class="line">a3 = sigmoid(z3);</span><br><span class="line">h = a3;</span><br><span class="line"></span><br><span class="line">yVec = zeros(m, num_labels);</span><br><span class="line"></span><br><span class="line">for i = 1:m</span><br><span class="line"></span><br><span class="line">  yVec(i, y(i)) = 1;</span><br><span class="line"></span><br><span class="line">endfor</span><br><span class="line"></span><br><span class="line">J = 1 / m * sum(sum(-1 * yVec .* log(h) - (1 - yVec) .* log(1 - h)));</span><br><span class="line"></span><br><span class="line">regularator = (lambda / (2 * m)) * (sum(sum(Theta1(:, 2:end) .^ 2)) + sum(sum(Theta2(:, 2:end) .^ 2)));</span><br><span class="line"></span><br><span class="line">J = J + regularator;</span><br><span class="line"></span><br><span class="line">% 反向傳播算法</span><br><span class="line"></span><br><span class="line">for t = 1:m</span><br><span class="line">  </span><br><span class="line">  % 設定 a1 為 X</span><br><span class="line">  a1 = [1; X(t,:)&apos;];</span><br><span class="line">  </span><br><span class="line">  % 計算 aL ( L = 1, 2, ..., L )</span><br><span class="line">  z2 = Theta1 * a1;</span><br><span class="line">  a2 = [1; sigmoid(z2)];</span><br><span class="line">  </span><br><span class="line">  z3 = Theta2 * a2;</span><br><span class="line">  a3 = sigmoid(z3);</span><br><span class="line">  </span><br><span class="line">  % 計算 deltaL</span><br><span class="line">  yy = ([1:num_labels] == y(t))&apos;;</span><br><span class="line">  delta3 = a3 - yy;</span><br><span class="line">  </span><br><span class="line">  % 計算 delta ( L - 1, L - 2, ..., 2 )</span><br><span class="line">  delta2 = (Theta2&apos; * delta3) .* [1; sigmoidGradient(z2)];</span><br><span class="line">  delta2 = delta2(2:end); % 不計算 bias unit</span><br><span class="line">  </span><br><span class="line">  % 計算 big delta</span><br><span class="line">  Theta1_grad = Theta1_grad + delta2 * a1&apos;;</span><br><span class="line">	Theta2_grad = Theta2_grad + delta3 * a2&apos;;</span><br><span class="line">  </span><br><span class="line">endfor</span><br><span class="line"></span><br><span class="line">Theta1_grad = (1/m) * Theta1_grad + (lambda/m) * [zeros(size(Theta1, 1), 1) Theta1(:,2:end)];</span><br><span class="line">Theta2_grad = (1/m) * Theta2_grad + (lambda/m) * [zeros(size(Theta2, 1), 1) Theta2(:,2:end)];</span><br><span class="line"></span><br><span class="line">% 展開</span><br><span class="line">grad = [Theta1_grad(:) ; Theta2_grad(:)];</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<ul>
<li>在 Octave 上的前向傳播與反向傳播</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">function g = sigmoidGradient(z)</span><br><span class="line"></span><br><span class="line">g = zeros(size(z));</span><br><span class="line"></span><br><span class="line">g = sigmoid(z) .* (1 - sigmoid(z));</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<ul>
<li><p>在 Octave 上的激勵函數的導函數</p>
</li>
<li><p><code>δⱼ⁽ˡ⁾</code> 就是 <code>aⱼ⁽ˡ⁾</code> 的偏差，也是代價函數的導數</p>
<ul>
<li><img src="https://i.imgur.com/1XpEolE.png" alt></li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/f82drs4.png" alt></p>
<ul>
<li><code>δ₁⁽⁴⁾ = a₁⁽⁴⁾ - y⁽ⁱ⁾</code></li>
<li><code>δ₂⁽³⁾ = Θ₁₂⁽³⁾ * δ₁⁽⁴⁾</code></li>
<li><code>δ₂⁽²⁾ = Θ₁₂⁽²⁾ * δ₁⁽³⁾ + Θ₂₂⁽²⁾ * δ₂⁽³⁾</code></li>
</ul>
<h3 id="實作反向傳播算法"><a href="#實作反向傳播算法" class="headerlink" title="實作反向傳播算法"></a>實作反向傳播算法</h3><h4 id="矩陣展開成向量與還原"><a href="#矩陣展開成向量與還原" class="headerlink" title="矩陣展開成向量與還原"></a>矩陣展開成向量與還原</h4><ul>
<li>在 <code>fminunc()</code> 的高級優化算法中，通常都需要傳入向量，這時我們就需要將矩陣展開成向量</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">thetaVector = [Theta1(:); Theta2(:); Theta3(:);]</span><br><span class="line">deltaVector = [D1(:); D2(:); D3(:)]</span><br></pre></td></tr></table></figure>

<ul>
<li>假設有 Θ⁽¹⁾、Θ⁽²⁾、Θ⁽³⁾，D⁽¹⁾、D⁽²⁾、D⁽³⁾，展開成向量</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Theta1 = reshape(thetaVector(1:110), 10, 11)</span><br><span class="line">Theta2 = reshape(thetaVector(111:220), 10, 11)</span><br><span class="line">Theta3 = reshape(thetaVector(221:231), 1, 11)</span><br></pre></td></tr></table></figure>

<ul>
<li>假設 Θ⁽¹⁾ 為 10 * 11 矩陣、Θ⁽²⁾ 為 10 * 11 矩陣、Θ⁽³⁾ 為 1 * 11 矩陣，將他們還原</li>
</ul>
<h5 id="應用"><a href="#應用" class="headerlink" title="應用"></a>應用</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">創建初始化的 Θ⁽¹⁾、Θ⁽²⁾、Θ⁽³⁾</span><br><span class="line">展開成向量 initialTheta</span><br><span class="line">fminunc(@costFunction, initialTheta, options)</span><br></pre></td></tr></table></figure>

<ul>
<li>使用 <code>fminunc()</code></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">function [jval, gradientVec] = costFunction(thetaVec)</span><br><span class="line"></span><br><span class="line">    還原 thetaVec 成 Θ⁽¹⁾、Θ⁽²⁾、Θ⁽³⁾</span><br><span class="line">    使用梯度下降或反向傳播算法算出 D⁽¹⁾、D⁽²⁾、D⁽³⁾ 和 J(Θ)</span><br><span class="line">    展開 D⁽¹⁾、D⁽²⁾、D⁽³⁾ 成向量 gradientVec</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<ul>
<li><code>costFunction()</code> 寫法</li>
</ul>
<h4 id="梯度驗證-Gradient-Checking"><a href="#梯度驗證-Gradient-Checking" class="headerlink" title="梯度驗證 ( Gradient Checking )"></a>梯度驗證 ( Gradient Checking )</h4><ul>
<li>通常使用反向傳播算法可能會出現許多 BUG，梯度驗證可以確保我們的反向傳播算法是如預期地工作的</li>
<li>算式 :</li>
</ul>
<p><img src="https://i.imgur.com/chqABww.png" alt></p>
<ul>
<li>有多個 Θ 矩陣時 :</li>
</ul>
<p><img src="https://i.imgur.com/WXnqaQ1.png" alt></p>
<ul>
<li>通常 <code>ϵ</code> 設定為 10⁻⁴，如果太小會導致數值問題</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">epsilon = 1e-4;</span><br><span class="line"></span><br><span class="line">for i = 1:n,</span><br><span class="line"></span><br><span class="line">  thetaPlus = theta;</span><br><span class="line">  thetaPlus(i) += epsilon;</span><br><span class="line">  </span><br><span class="line">  thetaMinus = theta;</span><br><span class="line">  thetaMinus(i) -= epsilon;</span><br><span class="line">  </span><br><span class="line">  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)</span><br><span class="line">  </span><br><span class="line">end;</span><br></pre></td></tr></table></figure>

<ul>
<li>在 Octave 上的梯度驗證</li>
<li>驗證 <code>gradApprox ≈ deltaVector</code>，確定反向傳播算法沒有錯誤後，就必須把梯度驗證關掉了，因為梯度驗證會導致程式執行非常的慢</li>
</ul>
<h4 id="隨機初始化-Random-Initialization"><a href="#隨機初始化-Random-Initialization" class="headerlink" title="隨機初始化 ( Random Initialization )"></a>隨機初始化 ( Random Initialization )</h4><ul>
<li>在神經網路中，將 Θ 的所有權重初始化為 0 是不適合的</li>
<li>這樣會因為對稱而造成所有節點更新為相同的值</li>
<li>所以我們需要使用下面方法來隨機初始化 ( 打破對稱 Symmetry breaking ) Θ 的權重 :</li>
</ul>
<p><img src="https://i.imgur.com/teXtQj5.png" alt></p>
<ul>
<li>將每個 <code>Θᵢⱼ⁽ˡ⁾</code> 初始化為 <code>[−ϵ, ϵ]</code> 之間的隨機值，此 <code>ϵ</code> 並非梯度驗證的 <code>ϵ</code></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.</span><br><span class="line"></span><br><span class="line">Theta1 = rand(10, 11) * (2 * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta2 = rand(10, 11) * (2 * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta3 = rand(1, 11) * (2 * INIT_EPSILON) - INIT_EPSILON;</span><br></pre></td></tr></table></figure>

<ul>
<li>在 Octave 上的隨機初始化概念</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">function W = randInitializeWeights(L_in, L_out)</span><br><span class="line"></span><br><span class="line">W = zeros(L_out, 1 + L_in);</span><br><span class="line"></span><br><span class="line">epsilon_init = 0.12;</span><br><span class="line">W = rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init; </span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<ul>
<li>在 Octave 上的隨機初始化</li>
</ul>
<h4 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h4><ol>
<li><p>神經網路的布局</p>
<ul>
<li><p>input 單元數 = 特徵數 ( x )</p>
</li>
<li><p>output 單元數 = 類別數 ( y )</p>
</li>
<li><p>hidden 單元數通常越多越好，但計算的成本通常會隨之增加，必須取得平衡</p>
</li>
<li><p>潛規則 : </p>
<ul>
<li>至少有一層 hidden layer</li>
<li>有兩層以上的 hidden layer，則建議每層單元數都相同</li>
</ul>
</li>
</ul>
</li>
<li><p>訓練神經網路</p>
<ol>
<li><p>隨機初始化權重 ( Θ )</p>
</li>
<li><p>實現向前傳播取得 hΘ(x⁽ⁱ⁾)</p>
</li>
<li><p>實現代價函數</p>
</li>
<li><p>實現反向傳播算法以計算導數 <img src="https://i.imgur.com/poAQcSC.png" alt></p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for i = 1:m,</span><br><span class="line">    </span><br><span class="line">    使用 (x⁽ⁱ⁾, y⁽ⁱ⁾) 執行向前傳播與反向傳播</span><br><span class="line">    取得 a⁽ˡ⁾ 與 delta d⁽ˡ⁾ ( l = 2, ..., L )</span><br><span class="line">    </span><br><span class="line">end;</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用梯度驗證確認反向傳播算法是否正確，<strong>確認後關閉</strong></p>
</li>
<li><p>使用梯度下降或其他最優化功能計算代價函數最小值，並獲得 Θ 的權重</p>
</li>
</ol>
</li>
</ol>
<h6 id="tags-筆記-機器學習-神經網路"><a href="#tags-筆記-機器學習-神經網路" class="headerlink" title="tags: 筆記 機器學習 神經網路"></a>tags: <code>筆記</code> <code>機器學習</code> <code>神經網路</code></h6></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Kenny Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://kennyliblog.nctu.me/2019/08/05/Machine-learning-neural-network-advanced/">https://kennyliblog.nctu.me/2019/08/05/Machine-learning-neural-network-advanced/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/筆記/">筆記    </a><a class="post-meta__tags" href="/tags/機器學習/">機器學習    </a><a class="post-meta__tags" href="/tags/神經網路/">神經網路    </a></div><div class="post_share"><div class="social-share" data-image="/img/note.jpg" data-sites="facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2019/07/26/Machine-learning-neural-network-representation/"><img class="next_cover lozad" data-src="/img/note.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>筆記 - 機器學習 神經網路 ( Neural Network ) - 表層結構</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/07/26/Machine-learning-neural-network-representation/" title="筆記 - 機器學習 神經網路 ( Neural Network ) - 表層結構"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 神經網路 ( Neural Network ) - 表層結構</div></a></div><div class="relatedPosts_item"><a href="/2019/07/24/Machine-learning-overfitting-problem/" title="筆記 - 機器學習 過度擬合問題 ( overfitting )"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 過度擬合問題 ( overfitting )</div></a></div><div class="relatedPosts_item"><a href="/2019/07/12/Machine-learning-basic-linear-regression/" title="筆記 - 機器學習 基礎與線性回歸"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 基礎與線性回歸</div></a></div><div class="relatedPosts_item"><a href="/2019/07/12/Octave-basic/" title="筆記 - Octave 基礎"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - Octave 基礎</div></a></div><div class="relatedPosts_item"><a href="/2019/07/19/Machine-learning-logistic-regression/" title="筆記 - 機器學習 邏輯回歸"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 邏輯回歸</div></a></div><div class="relatedPosts_item"><a href="/2019/06/17/CSS-basic/" title="筆記 - CSS 基礎"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - CSS 基礎</div></a></div></div><div class="clear_both"></div></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 By Kenny Li</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="Read Mode"> </i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion">繁</a><i class="fa fa-moon-o nightshift" id="nightshift" title="Dark Mode"></i></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/baidupush.js"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script></body></html>