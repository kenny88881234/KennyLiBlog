<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>筆記 - 機器學習 有效的改善模型 | Kenny's Blog</title><meta name="description" content="筆記 - 機器學習 有效的改善模型"><meta name="keywords" content="筆記,機器學習,改善模型"><meta name="author" content="Kenny Li"><meta name="copyright" content="Kenny Li"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="https://kennyliblog.nctu.me/2019/08/08/Machine-learning-advice-for-model/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="筆記 - 機器學習 有效的改善模型"><meta name="twitter:description" content="筆記 - 機器學習 有效的改善模型"><meta name="twitter:image" content="/img/note.jpg"><meta property="og:type" content="article"><meta property="og:title" content="筆記 - 機器學習 有效的改善模型"><meta property="og:url" content="https://kennyliblog.nctu.me/2019/08/08/Machine-learning-advice-for-model/"><meta property="og:site_name" content="Kenny's Blog"><meta property="og:description" content="筆記 - 機器學習 有效的改善模型"><meta property="og:image" content="/img/note.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="筆記 - 機器學習 系統的設計" href="https://kennyliblog.nctu.me/2019/08/09/Machine-learning-system-design/"><link rel="next" title="筆記 - 機器學習 神經網路 - 進階學習" href="https://kennyliblog.nctu.me/2019/08/05/Machine-learning-neural-network-advanced/"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"cookieDomain":"https://KennyLi.me/","msgToTraditionalChinese":"简","msgToSimplifiedChinese":"繁"},
  highlight_copy: 'true',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Bookmark',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#筆記-機器學習-有效的改善模型"><span class="toc-number">1.</span> <span class="toc-text">[筆記] 機器學習 有效的改善模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#改善方法"><span class="toc-number">1.0.1.</span> <span class="toc-text">改善方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#機器學習診斷法-Machine-learning-diagnostic"><span class="toc-number">1.0.2.</span> <span class="toc-text">機器學習診斷法 ( Machine learning diagnostic )</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#評估假設-Evaluating-a-Hypothesis"><span class="toc-number">1.0.2.1.</span> <span class="toc-text">評估假設 ( Evaluating a Hypothesis )</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#模型選擇-Model-selection"><span class="toc-number">1.0.2.2.</span> <span class="toc-text">模型選擇 ( Model selection )</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#分辨偏差與方差-Bias-vs-Variance"><span class="toc-number">1.0.2.3.</span> <span class="toc-text">分辨偏差與方差 ( Bias vs. Variance )</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#如何選擇-λ"><span class="toc-number">1.0.2.4.</span> <span class="toc-text">如何選擇 λ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#學習曲線"><span class="toc-number">1.0.2.5.</span> <span class="toc-text">學習曲線</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#總結"><span class="toc-number">1.0.3.</span> <span class="toc-text">總結</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#tags-筆記-機器學習-改善模型"><span class="toc-number">1.0.3.0.0.1.</span> <span class="toc-text">tags: 筆記 機器學習 改善模型</span></a></li></ol></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/note.jpg)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">Kenny's Blog</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">筆記 - 機器學習 有效的改善模型</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2019-08-08<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2019-08-09</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/筆記/">筆記</a></span></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="筆記-機器學習-有效的改善模型"><a href="#筆記-機器學習-有效的改善模型" class="headerlink" title="[筆記] 機器學習 有效的改善模型"></a>[筆記] 機器學習 有效的改善模型</h1><h3 id="改善方法"><a href="#改善方法" class="headerlink" title="改善方法"></a>改善方法</h3><ul>
<li>取得更多訓練資料</li>
<li>嘗試較少的特徵量</li>
<li>嘗試較多的特徵量</li>
<li>嘗試多項式的特徵量</li>
<li>嘗試增加 λ</li>
<li>嘗試減少 λ</li>
</ul>
<h3 id="機器學習診斷法-Machine-learning-diagnostic"><a href="#機器學習診斷法-Machine-learning-diagnostic" class="headerlink" title="機器學習診斷法 ( Machine learning diagnostic )"></a>機器學習診斷法 ( Machine learning diagnostic )</h3><h4 id="評估假設-Evaluating-a-Hypothesis"><a href="#評估假設-Evaluating-a-Hypothesis" class="headerlink" title="評估假設 ( Evaluating a Hypothesis )"></a>評估假設 ( Evaluating a Hypothesis )</h4><ul>
<li><p>我們的假設可能使訓練樣本的誤差較小，但仍然不準確 ( 因為過度擬合 )，這時就必須評估假設</p>
</li>
<li><p>為了評估假設，將訓練樣本分為兩組 :</p>
<ol>
<li>70% 的隨機數據當訓練集</li>
<li>30% 的隨機數據當測試集</li>
</ol>
</li>
<li><p>過程 :</p>
<ol>
<li>使用新的訓練集學習出 θ 和最小化的 J<sub>train</sub>(θ)</li>
<li>使用新的測試集計算出測試集錯誤率 ( The test set error ) J<sub>test</sub>(θ)</li>
</ol>
</li>
<li><p>測試集錯誤率 ( The test set error )</p>
</li>
</ul>
<ol>
<li><p>線性回歸 :</p>
<p> <img src="https://i.imgur.com/RESZL8X.png" alt></p>
</li>
<li><p>邏輯回歸 :</p>
<p> <img src="https://i.imgur.com/sRZ5b3F.png" alt></p>
<ul>
<li><p>判斷是否錯誤</p>
<p><img src="https://i.imgur.com/7P8RcuT.png" alt></p>
</li>
<li><p>算出錯誤率</p>
</li>
</ul>
</li>
</ol>
<h4 id="模型選擇-Model-selection"><a href="#模型選擇-Model-selection" class="headerlink" title="模型選擇 ( Model selection )"></a>模型選擇 ( Model selection )</h4><ul>
<li><p>鑑於具有需多不同的多項式模型，我們必須要有方法來做模型的選擇</p>
</li>
<li><p>將訓練樣本分為三組 :</p>
<ol>
<li>60% 的隨機數據當訓練集</li>
<li>20% 的隨機數據當交叉驗證集 ( cross validation set )</li>
<li>20% 的隨機數據當測試集</li>
</ol>
</li>
<li><p>過程 :</p>
<ol>
<li>使用新的訓練集來學習出每個多項式的 θ 和最小化的 J<sub>train</sub>(θ)</li>
<li>使用新的交叉驗證集來計算出交叉驗證集錯誤率，找出錯誤率最小的模型 d</li>
<li>最後使用新的測試集算出泛化誤差 J<sub>test</sub>(θ(d))</li>
</ol>
</li>
<li><p>此方法，測試集並沒有參與訓練</p>
</li>
</ul>
<h4 id="分辨偏差與方差-Bias-vs-Variance"><a href="#分辨偏差與方差-Bias-vs-Variance" class="headerlink" title="分辨偏差與方差 ( Bias vs. Variance )"></a>分辨偏差與方差 ( Bias vs. Variance )</h4><p><img src="https://i.imgur.com/yIcEqii.png" alt></p>
<ul>
<li><p>高偏差 ( High bias ) 屬於擬合不足 ( underfitting ) :</p>
<ul>
<li>J<sub>train</sub>(θ)、J<sub>CV</sub>(θ) 都很高</li>
<li>J<sub>train</sub>(θ) ≈ J<sub>CV</sub>(θ)</li>
</ul>
</li>
<li><p>高方差 ( High Variance ) 屬於過度擬合 ( overfitting ) :</p>
<ul>
<li>J<sub>train</sub>(θ) 較低</li>
<li>J<sub>CV</sub>(θ) &gt;&gt; J<sub>train</sub>(θ)</li>
</ul>
</li>
</ul>
<h4 id="如何選擇-λ"><a href="#如何選擇-λ" class="headerlink" title="如何選擇 λ"></a>如何選擇 λ</h4><p><img src="https://i.imgur.com/tCVgnVa.png" alt></p>
<ul>
<li><p>當 λ 過大時，會導致擬合不足，而當 λ 過小時，會導致過度擬合</p>
</li>
<li><p>如何選擇模型與 λ :</p>
<ol>
<li>先列出一個 λ 列表 ( 例如 : λ ∈ {0, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28, 2.56, 5.12, 10.24} )</li>
<li>建立一組擁有多項式的模型</li>
<li>使用每個 λ，各學習出 θ ( 使用正規化學習 )</li>
<li>使用交叉驗證集來算出錯誤率 J<sub>CV</sub>(θ) ( 不使用正規化，或使 λ = 0 )</li>
<li>選擇 J<sub>CV</sub>(θ) 最低的作為最佳組合</li>
<li>使用此最佳組合 θ、λ，查看在 J<sub>test</sub>(θ) 上是否運作良好</li>
</ol>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">function [lambda_vec, error_train, error_val] = validationCurve(X, y, Xval, yval)</span><br><span class="line"></span><br><span class="line">% λ 列表</span><br><span class="line">lambda_vec = [0 0.001 0.003 0.01 0.03 0.1 0.3 1 3 10]&apos;;</span><br><span class="line"></span><br><span class="line">error_train = zeros(length(lambda_vec), 1);</span><br><span class="line">error_val = zeros(length(lambda_vec), 1);</span><br><span class="line"></span><br><span class="line">for i = 1:length(lambda_vec)</span><br><span class="line"></span><br><span class="line">  % 使用每個 λ，各學習出 θ</span><br><span class="line">  lambda = lambda_vec(i);</span><br><span class="line">  theta = trainLinearReg(X, y, lambda);</span><br><span class="line">  </span><br><span class="line">  % 計算出訓練集與交叉驗證集的錯誤率</span><br><span class="line">  error_train(i) = linearRegCostFunction(X, y, theta, 0);</span><br><span class="line">  error_val(i) = linearRegCostFunction(Xval, yval, theta, 0);</span><br><span class="line"></span><br><span class="line">endfor</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<ul>
<li>在 Octave 上的選擇 λ 函式</li>
</ul>
<h4 id="學習曲線"><a href="#學習曲線" class="headerlink" title="學習曲線"></a>學習曲線</h4><ul>
<li><p>在極少數資料 ( 1、2 或 3 ) 的訓練集，容易產生錯誤率為 0，因為很容易找到符合這些資料的多項式，因此 :</p>
<ul>
<li>隨訓練資料越多，函數的誤差增加</li>
<li>在到達某個資料量後，誤差值將穩定下來</li>
</ul>
</li>
<li><p>高偏差 ( High bias ) :</p>
<ul>
<li><p>資料量少 : J<sub>train</sub>(θ) 較低，J<sub>CV</sub>(θ) 較高</p>
</li>
<li><p>資料量多 : J<sub>train</sub>(θ) ≈ J<sub>CV</sub>(θ)，都很高</p>
</li>
<li><p>如果繼續增加訓練資料，並沒有太大改善</p>
<p><img src="https://i.imgur.com/YKmkZIS.png" alt></p>
</li>
</ul>
</li>
<li><p>高方差 ( High Variance ) :</p>
<ul>
<li><p>資料量少 : J<sub>train</sub>(θ) 較低，J<sub>CV</sub>(θ) 較高</p>
</li>
<li><p>資料量多 : J<sub>train</sub>(θ) 隨資料量越大而增加， J<sub>CV</sub>(θ) 隨著減少且不平穩，J<sub>train</sub>(θ) &lt; J<sub>CV</sub>(θ)，兩個差異仍然很大</p>
</li>
<li><p>如果繼續增加訓練資料，有助於改善</p>
<p><img src="https://i.imgur.com/0Ma9WVM.png" alt></p>
</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">function [error_train, error_val] = learningCurve(X, y, Xval, yval, lambda)</span><br><span class="line"></span><br><span class="line">m = size(X, 1);</span><br><span class="line"></span><br><span class="line">error_train = zeros(m, 1);</span><br><span class="line">error_val = zeros(m, 1);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for i = 1:m</span><br><span class="line"></span><br><span class="line">  X_train = X(1:i, :);</span><br><span class="line">  y_train = y(1:i);</span><br><span class="line">  </span><br><span class="line">  % 訓練出 θ</span><br><span class="line">  theta = trainLinearReg(X_train, y_train, lambda);</span><br><span class="line">  </span><br><span class="line">  % 計算出訓練集與測試集的錯誤率</span><br><span class="line">  error_train(i) = linearRegCostFunction(X_train, y_train, theta, 0);</span><br><span class="line">  error_val(i) = linearRegCostFunction(Xval, yval, theta, 0);</span><br><span class="line"></span><br><span class="line">endfor</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<ul>
<li>在 Octave 上的學習曲線函式</li>
</ul>
<h3 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h3><ul>
<li><p>解決高偏差 ( High bias ) :</p>
<ul>
<li><p>嘗試較多的特徵量</p>
</li>
<li><p>嘗試多項式的特徵量</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">function [X_poly] = polyFeatures(X, p)</span><br><span class="line"></span><br><span class="line">X_poly = zeros(numel(X), p);</span><br><span class="line"></span><br><span class="line">X_poly(:, 1) = X;</span><br><span class="line"></span><br><span class="line">for i = 2:p</span><br><span class="line"></span><br><span class="line">  X_poly(:, i) = X .* X_poly(:, i - 1)</span><br><span class="line"></span><br><span class="line">endfor</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<ul>
<li>在 Octave 上算出單特徵量多次方的函式</li>
</ul>
</li>
<li><p>嘗試減少 λ</p>
</li>
</ul>
</li>
<li><p>解決高方差 ( High Variance ) :</p>
<ul>
<li>取得更多訓練資料</li>
<li>嘗試較少的特徵量</li>
<li>嘗試增加 λ</li>
</ul>
</li>
<li><p>診斷神經網路</p>
<ul>
<li><p>較少參數的神經網路</p>
<ul>
<li>容易產生擬合不足 ( underfitting )</li>
<li>計算成本較低</li>
</ul>
</li>
<li><p>較多參數的神經網路</p>
<ul>
<li>容易產生過度擬合 ( overfitting )</li>
<li>計算成本較高</li>
<li>可以使用正規化來解決過度擬合</li>
</ul>
</li>
<li><p>如何選擇 Hidden layer 數量</p>
<ul>
<li>使用多種 Hidden layer 數量不同的模型</li>
<li>再用交叉驗證集選出效果較佳的</li>
</ul>
</li>
</ul>
</li>
<li><p>模型複雜度</p>
<ul>
<li><p>模型複雜度低</p>
<ul>
<li>高偏差、低方差</li>
<li>對於訓練數據和測試數據都有較大的誤差</li>
</ul>
</li>
<li><p>模型複雜度高</p>
<ul>
<li>低偏差、高方差</li>
<li>非常擬合訓練數據，但對測試數據有較大的誤差</li>
</ul>
</li>
</ul>
</li>
</ul>
<h6 id="tags-筆記-機器學習-改善模型"><a href="#tags-筆記-機器學習-改善模型" class="headerlink" title="tags: 筆記 機器學習 改善模型"></a>tags: <code>筆記</code> <code>機器學習</code> <code>改善模型</code></h6></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Kenny Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://kennyliblog.nctu.me/2019/08/08/Machine-learning-advice-for-model/">https://kennyliblog.nctu.me/2019/08/08/Machine-learning-advice-for-model/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/筆記/">筆記    </a><a class="post-meta__tags" href="/tags/機器學習/">機器學習    </a><a class="post-meta__tags" href="/tags/改善模型/">改善模型    </a></div><div class="post_share"><div class="social-share" data-image="/img/note.jpg" data-sites="facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-left"><a href="/2019/08/09/Machine-learning-system-design/"><img class="prev_cover lozad" data-src="/img/note.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Previous Post</div><div class="prev_info"><span>筆記 - 機器學習 系統的設計</span></div></a></div><div class="next-post pull-right"><a href="/2019/08/05/Machine-learning-neural-network-advanced/"><img class="next_cover lozad" data-src="/img/note.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>筆記 - 機器學習 神經網路 - 進階學習</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/08/09/Machine-learning-system-design/" title="筆記 - 機器學習 系統的設計"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 系統的設計</div></a></div><div class="relatedPosts_item"><a href="/2019/07/12/Octave-basic/" title="筆記 - Octave 基礎"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - Octave 基礎</div></a></div><div class="relatedPosts_item"><a href="/2019/08/05/Machine-learning-neural-network-advanced/" title="筆記 - 機器學習 神經網路 - 進階學習"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 神經網路 - 進階學習</div></a></div><div class="relatedPosts_item"><a href="/2019/07/19/Machine-learning-logistic-regression/" title="筆記 - 機器學習 邏輯回歸"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 邏輯回歸</div></a></div><div class="relatedPosts_item"><a href="/2019/07/12/Machine-learning-basic-linear-regression/" title="筆記 - 機器學習 基礎與線性回歸"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 基礎與線性回歸</div></a></div><div class="relatedPosts_item"><a href="/2019/07/24/Machine-learning-overfitting-problem/" title="筆記 - 機器學習 過度擬合問題 ( overfitting )"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 過度擬合問題 ( overfitting )</div></a></div></div><div class="clear_both"></div></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 By Kenny Li</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="Read Mode"> </i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion">繁</a><i class="fa fa-moon-o nightshift" id="nightshift" title="Dark Mode"></i></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/baidupush.js"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script></body></html>