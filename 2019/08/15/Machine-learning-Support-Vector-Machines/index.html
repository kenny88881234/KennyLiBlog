<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>筆記 - 機器學習 支持向量機 ( Support Vector Machines ) | Kenny's Blog</title><meta name="description" content="筆記 - 機器學習 支持向量機 ( Support Vector Machines )"><meta name="keywords" content="筆記,機器學習,支持向量機SVM"><meta name="author" content="Kenny Li"><meta name="copyright" content="Kenny Li"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="https://kennyliblog.nctu.me/2019/08/15/Machine-learning-Support-Vector-Machines/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="筆記 - 機器學習 支持向量機 ( Support Vector Machines )"><meta name="twitter:description" content="筆記 - 機器學習 支持向量機 ( Support Vector Machines )"><meta name="twitter:image" content="/img/note.jpg"><meta property="og:type" content="article"><meta property="og:title" content="筆記 - 機器學習 支持向量機 ( Support Vector Machines )"><meta property="og:url" content="https://kennyliblog.nctu.me/2019/08/15/Machine-learning-Support-Vector-Machines/"><meta property="og:site_name" content="Kenny's Blog"><meta property="og:description" content="筆記 - 機器學習 支持向量機 ( Support Vector Machines )"><meta property="og:image" content="/img/note.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="next" title="問題 - 如何在 Hexo 上使用數學式" href="https://kennyliblog.nctu.me/2019/08/13/Hexo-use-mathJax/"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"cookieDomain":"https://KennyLi.me/","msgToTraditionalChinese":"简","msgToSimplifiedChinese":"繁"},
  highlight_copy: 'true',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Bookmark',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#筆記-機器學習-支持向量機-Support-Vector-Machines"><span class="toc-number">1.</span> <span class="toc-text">[筆記] 機器學習 支持向量機 ( Support Vector Machines )</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SVM-的代價函數"><span class="toc-number">1.0.1.</span> <span class="toc-text">SVM 的代價函數</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SVM-的推導"><span class="toc-number">1.0.2.</span> <span class="toc-text">SVM 的推導</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#內積"><span class="toc-number">1.0.2.1.</span> <span class="toc-text">內積</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SVM-如何選擇決策邊界"><span class="toc-number">1.0.2.2.</span> <span class="toc-text">SVM 如何選擇決策邊界</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#核函數-Kernels"><span class="toc-number">1.0.3.</span> <span class="toc-text">核函數 ( Kernels )</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#新的特徵值"><span class="toc-number">1.0.3.1.</span> <span class="toc-text">新的特徵值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#核函數與相似度函數"><span class="toc-number">1.0.3.2.</span> <span class="toc-text">核函數與相似度函數</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#應用示例"><span class="toc-number">1.0.3.3.</span> <span class="toc-text">應用示例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#如何選擇標記點"><span class="toc-number">1.0.3.4.</span> <span class="toc-text">如何選擇標記點</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#支持向量機結合核函數"><span class="toc-number">1.0.3.5.</span> <span class="toc-text">支持向量機結合核函數</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#如何選擇支持向量機中的參數"><span class="toc-number">1.0.3.6.</span> <span class="toc-text">如何選擇支持向量機中的參數</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用支持向量機-SVM"><span class="toc-number">1.0.4.</span> <span class="toc-text">使用支持向量機 ( SVM )</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#高斯核函數-Gaussian-Kernel"><span class="toc-number">1.0.4.1.</span> <span class="toc-text">高斯核函數 ( Gaussian Kernel )</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#其他的核函數"><span class="toc-number">1.0.4.2.</span> <span class="toc-text">其他的核函數</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#多類別分類"><span class="toc-number">1.0.4.3.</span> <span class="toc-text">多類別分類</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#邏輯回歸-vs-SVMs"><span class="toc-number">1.0.4.4.</span> <span class="toc-text">邏輯回歸 vs SVMs</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#tags-筆記-機器學習-支持向量機SVM"><span class="toc-number">1.0.4.4.0.1.</span> <span class="toc-text">tags: 筆記 機器學習 支持向量機SVM</span></a></li></ol></li></ol></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/note.jpg)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">Kenny's Blog</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">筆記 - 機器學習 支持向量機 ( Support Vector Machines )</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2019-08-15<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2019-08-15</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/筆記/">筆記</a></span></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h1 id="筆記-機器學習-支持向量機-Support-Vector-Machines"><a href="#筆記-機器學習-支持向量機-Support-Vector-Machines" class="headerlink" title="[筆記] 機器學習 支持向量機 ( Support Vector Machines )"></a>[筆記] 機器學習 支持向量機 ( Support Vector Machines )</h1><ul>
<li>簡稱 SVM</li>
<li>屬於監督式學習算法</li>
<li>主要用於找到一個決策邊界 ( decision boundary ) 讓兩類的邊界 ( margins ) 最大化</li>
<li>大間距分類器 ( Large margin classifiers )</li>
</ul>
<h3 id="SVM-的代價函數"><a href="#SVM-的代價函數" class="headerlink" title="SVM 的代價函數"></a>SVM 的代價函數</h3><ul>
<li><p><img src="https://i.imgur.com/VKJLCuF.png" alt></p>
<ul>
<li>將 <code>邏輯回歸的代價函數</code> * $\dfrac{m}{λ}$</li>
<li>可以將 <code>C</code> 看成 $\dfrac{1}{λ}$，優化目標相同，SVM 的 <code>C</code> 與邏輯函數的 <code>λ</code> 只是透過不同方法來控制權重</li>
</ul>
</li>
<li><p>這裡 h<sub>θ</sub>(x) 的定義 :</p>
<ul>
<li>h<sub>θ</sub>(x) = 1，if θ<sup>T</sup>x ≥ 0</li>
<li>h<sub>θ</sub>(x) = 0，otherwise</li>
</ul>
</li>
<li><p>cost<sub>1</sub>(z) 與 cost<sub>0</sub>(z) 的圖 :</p>
<p>  <img src="https://i.imgur.com/V8Cm2zU.png" alt></p>
<ul>
<li><p>支持向量機的條件更加嚴格 :</p>
<ul>
<li>h<sub>θ</sub>(x) = 1，if θ<sup>T</sup>x ≥ 1</li>
<li>h<sub>θ</sub>(x) = 0，if θ<sup>T</sup>x ≤ -1</li>
</ul>
</li>
</ul>
</li>
<li><p><code>C</code> 值的大小 :</p>
<ul>
<li><p>當 <code>C</code> 非常大 :</p>
<ul>
<li>相當於 <code>λ</code> 非常小</li>
<li>對於誤差點，也會進行很好的擬合</li>
<li>造成過度擬合的情況</li>
</ul>
</li>
<li><p>當 <code>C</code> 非常小 :</p>
<ul>
<li>相當於 <code>λ</code> 非常大</li>
<li>降低過度擬合的情況</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="SVM-的推導"><a href="#SVM-的推導" class="headerlink" title="SVM 的推導"></a>SVM 的推導</h3><h4 id="內積"><a href="#內積" class="headerlink" title="內積"></a>內積</h4><p><img src="https://i.imgur.com/3seajNr.png" alt></p>
<ul>
<li>$||u||$ = 向量長度 = $\sqrt{u_1^2 + u_2^2}$</li>
<li>p = v 向量投影在 u 向量的長度 ( 當 u、v 夾角大於 90 度時，p 就會是負的 )</li>
<li>u<sup>T</sup>v = $p \cdot ||u||$ = u<sub>1</sub>v<sub>1</sub> + u<sub>2</sub>v<sub>2</sub></li>
</ul>
<h4 id="SVM-如何選擇決策邊界"><a href="#SVM-如何選擇決策邊界" class="headerlink" title="SVM 如何選擇決策邊界"></a>SVM 如何選擇決策邊界</h4><ul>
<li><img src="https://i.imgur.com/HbOSutr.png" alt> = $\dfrac{1}{2}(θ_1^2 + θ_2^2)$ = $\dfrac{1}{2}(\sqrt{θ_1^2 + θ_2^2})^2$ = $\dfrac{1}{2}||θ||^2$</li>
<li><p>θ<sup>T</sup>x<sup>(i)</sup> = $p^{(i)} \cdot ||θ||$ = $θ_1x_1^{(i)} + θ_2x_2^{(i)}$</p>
<ul>
<li>θ<sup>T</sup>x<sup>(i)</sup> = $p^{(i)} \cdot ||θ||$ ≥ 1，if $y^{(i)}$ = 1</li>
<li>θ<sup>T</sup>x<sup>(i)</sup> = $p^{(i)} \cdot ||θ||$ ≤ -1，if $y^{(i)}$ = 0</li>
</ul>
</li>
</ul>
<p>首先要知道向量 θ 會與決策邊界垂直，且 θ<sub>0</sub> = 0 表示決策邊界通過原點，糟糕的決策邊界與良好的決策邊界如下 :</p>
<p><img src="https://i.imgur.com/G5ldInk.png" alt></p>
<ul>
<li><p>此為糟糕的決策邊界 ( 綠線 ) 與其向量 θ ( 藍線 )</p>
<ul>
<li>將每個 x<sup>(i)</sup> 投影在向量 θ 上，得到 p<sup>(i)</sup></li>
<li>當 p<sup>(i)</sup> 都較小時，為了滿足 $p^{(i)} \cdot ||θ||$ ≥ 1，$||θ||$ 就必須非常大</li>
<li>但當 $||θ||$ 非常大時，就會使 $\dfrac{1}{2}||θ||^2$ 也就是代價函數跟著變大，所以 SVM 就不會選擇此決策邊界</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/9Dn6Ohw.png" alt></p>
<ul>
<li><p>此為良好的決策邊界 ( 綠線 ) 與其向量 θ ( 藍線 )</p>
<ul>
<li>同上得到 p<sup>(i)</sup></li>
<li>當 p<sup>(i)</sup> 都較大時，$||θ||$ 就可以小一點</li>
<li>$||θ||$ 較小時，$\dfrac{1}{2}||θ||^2$ 也就是代價函數跟著變小，所以 SVM 就會選擇此決策邊界</li>
</ul>
</li>
</ul>
<h3 id="核函數-Kernels"><a href="#核函數-Kernels" class="headerlink" title="核函數 ( Kernels )"></a>核函數 ( Kernels )</h3><ul>
<li>用來打造非線性的支持向量機</li>
</ul>
<h4 id="新的特徵值"><a href="#新的特徵值" class="headerlink" title="新的特徵值"></a>新的特徵值</h4><ul>
<li><p>x 與標記點 ( l<sup>(1)</sup>、l<sup>(2)</sup>、l<sup>(3)</sup>、… ) 通過相似度函數計算出新的特徵值 f<sub>1</sub>、f<sub>2</sub>、f<sub>3</sub>、…</p>
<ul>
<li>f<sub>1</sub> = Similarity(x, l<sup>(1)</sup>) = $exp(-\dfrac{||x - l^{(1)}||^2}{2σ^2})$</li>
<li>f<sub>2</sub> = Similarity(x, l<sup>(2)</sup>) = $exp(-\dfrac{||x - l^{(2)}||^2}{2σ^2})$</li>
<li>f<sub>3</sub> = Similarity(x, l<sup>(3)</sup>) = $exp(-\dfrac{||x - l^{(3)}||^2}{2σ^2})$</li>
</ul>
</li>
<li><p>相似度函數是一種核函數，這裡用的是高斯核函數 ( Gaussian Kernel )</p>
</li>
</ul>
<h4 id="核函數與相似度函數"><a href="#核函數與相似度函數" class="headerlink" title="核函數與相似度函數"></a>核函數與相似度函數</h4><ul>
<li><p>f<sub>1</sub> = Similarity(x, l<sup>(1)</sup>) = $exp(-\dfrac{||x - l^{(1)}||^2}{2σ^2})$</p>
</li>
<li><p>如果 x ≈ l<sup>(1)</sup> :</p>
<ul>
<li>f<sub>1</sub> ≈ $exp(-\dfrac{0^2}{2σ^2})$ ≈ 1</li>
</ul>
</li>
<li><p>如果 x 與 l<sup>(1)</sup> 相差很遠 :</p>
<ul>
<li>f<sub>1</sub> ≈ $exp(-\dfrac{(Large Number)^2}{2σ^2})$ ≈ 0</li>
</ul>
</li>
<li><p>$σ^2$ 較小 :</p>
<p>  <img src="https://i.imgur.com/pGAfUsU.png" alt></p>
<ul>
<li>當遠離 l<sup>(1)</sup> 時，f<sub>1</sub> 下降較快</li>
</ul>
</li>
<li><p>$σ^2$ 較大 :</p>
<p>  <img src="https://i.imgur.com/OSC2kmI.png" alt></p>
<ul>
<li>當遠離 l<sup>(1)</sup> 時，f<sub>1</sub> 下降較慢</li>
</ul>
</li>
</ul>
<h4 id="應用示例"><a href="#應用示例" class="headerlink" title="應用示例"></a>應用示例</h4><p><img src="https://i.imgur.com/WWV1ZQh.png" alt></p>
<ul>
<li>假設預測 y = 1，如果 θ<sub>0</sub> + θ<sub>1</sub>f<sub>1</sub> + θ<sub>2</sub>f<sub>2</sub> + θ<sub>3</sub>f<sub>3</sub> ≥ 0</li>
<li><p>θ<sub>0</sub> = -0.5，θ<sub>1</sub> = 1，θ<sub>2</sub> = 1，θ<sub>3</sub> = 0</p>
<ul>
<li><p>新的樣本 x 與標記點 l<sup>(1)</sup> 相近</p>
<ul>
<li>f<sub>1</sub> ≈ 1，f<sub>2</sub> ≈ 0，f<sub>3</sub> ≈ 0</li>
<li>θ<sub>0</sub> + θ<sub>1</sub> <em> 1 + θ<sub>2</sub> </em> 0 + θ<sub>3</sub> * 0 = -0.5 + 1 = 0.5 ≥ 0</li>
<li>預測 y = 1</li>
</ul>
</li>
<li><p>新的樣本 x 與標記點 l<sup>(3)</sup> 相近</p>
<ul>
<li>f<sub>1</sub> ≈ 0，f<sub>2</sub> ≈ 0，f<sub>3</sub> ≈ 1</li>
<li>θ<sub>0</sub> + θ<sub>1</sub> <em> 0 + θ<sub>2</sub> </em> 0 + θ<sub>3</sub> * 1 = -0.5 + 0 = -0.5 ≤ 0</li>
<li>預測 y = 0</li>
</ul>
</li>
<li><p>新的樣本 x 與所有標記點都相遠</p>
<ul>
<li>f<sub>1</sub> ≈ 0，f<sub>2</sub> ≈ 0，f<sub>3</sub> ≈ 0</li>
<li>θ<sub>0</sub> + θ<sub>1</sub> <em> 0 + θ<sub>2</sub> </em> 0 + θ<sub>3</sub> * 0 = -0.5 + 0 = -0.5 ≤ 0</li>
<li>預測 y = 0</li>
</ul>
</li>
</ul>
</li>
<li><p>結論 :</p>
<ul>
<li><p>只要靠近標記點 l<sup>(1)</sup>、l<sup>(2)</sup> 就預測 y = 1，可以看出決策邊界如下 :</p>
<p><img src="https://i.imgur.com/htWXXK1.png" alt></p>
</li>
</ul>
</li>
</ul>
<h4 id="如何選擇標記點"><a href="#如何選擇標記點" class="headerlink" title="如何選擇標記點"></a>如何選擇標記點</h4><ul>
<li>把每個訓練資料看作是一個標記點 ( landmark )，所以會有 m 個標記點</li>
</ul>
<p><img src="https://i.imgur.com/ylbcFRQ.png" alt></p>
<h4 id="支持向量機結合核函數"><a href="#支持向量機結合核函數" class="headerlink" title="支持向量機結合核函數"></a>支持向量機結合核函數</h4><ol>
<li>使用 m 個資料 x<sup>(1)</sup> ~ x<sup>(m)</sup> 選擇出標記點 l<sup>(1)</sup> = x<sup>(1)</sup> ~ l<sup>(m)</sup> = x<sup>(m)</sup></li>
<li><p>算出新的特徵量 f<sub>1</sub><sup>(i)</sup> ~ f<sub>m</sub><sup>(i)</sup>，每個特徵量 x<sup>(i)</sup> 會算出 m 個 f<sup>(i)</sup></p>
<p> f<sub>1</sub><sup>(i)</sup> = Similarity(x<sup>(i)</sup>, l<sup>(1)</sup>)<br> f<sub>2</sub><sup>(i)</sup> = Similarity(x<sup>(i)</sup>, l<sup>(2)</sup>)<br> …<br> f<sub>m</sub><sup>(i)</sup> = Similarity(x<sup>(i)</sup>, l<sup>(m)</sup>)</p>
</li>
<li><p>訓練出 θ</p>
<p> <img src="https://i.imgur.com/piBlgXP.png" alt></p>
<ul>
<li>上面的 n = m</li>
<li><p><img src="https://i.imgur.com/CrRNVdA.png" alt> = θ<sup>T</sup>θ</p>
<ul>
<li>在實際運作上可能會是 θ<sup>T</sup> 乘上某個依賴核函數的矩陣再乘以 θ</li>
<li>目的是使支持向量機能更有效率的運行</li>
</ul>
</li>
</ul>
</li>
<li><p>預測</p>
<ul>
<li>如果 θ<sup>T</sup>f ≥ 0，預測 y = 1</li>
</ul>
</li>
</ol>
<p>P.S. 為什麼不在其他算法上使用核函數的概念</p>
<ul>
<li>事實上是可以的，但會十分緩慢</li>
<li>因為支持向量機的設計細節上可以很適合核函數，但其他算法並沒有</li>
</ul>
<h4 id="如何選擇支持向量機中的參數"><a href="#如何選擇支持向量機中的參數" class="headerlink" title="如何選擇支持向量機中的參數"></a>如何選擇支持向量機中的參數</h4><ul>
<li><p><code>C</code> ( = $\dfrac{1}{λ}$ ) :</p>
<ul>
<li>較大的 <code>C</code> : 相當於較小的 <code>λ</code>，低偏差 ( bias )，高方差 ( variance )，過擬合</li>
<li>較小的 <code>C</code> : 相當於較大的 <code>λ</code>，高偏差 ( bias )，低方差 ( variance )，欠擬合</li>
</ul>
</li>
<li><p>$σ^2$</p>
<ul>
<li>較大的 $σ^2$ : 特徵量 f<sub>i</sub> 的變化較平滑，高偏差 ( bias )，低方差 ( variance )</li>
<li>較小的 $σ^2$ : 特徵量 f<sub>i</sub> 的變化較不平滑，低偏差 ( bias )，高方差 ( variance )</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">function [C, sigma] = dataset3Params(X, y, Xval, yval)</span><br><span class="line"></span><br><span class="line">C_trial = [0.01 0.03 0.1 0.3 1 3 10 30];</span><br><span class="line">sigma_trial = C_trial;</span><br><span class="line"></span><br><span class="line">m = size(C_trial, 2);</span><br><span class="line"></span><br><span class="line">% error 最大就是 1</span><br><span class="line">initial_error = 1;</span><br><span class="line"></span><br><span class="line">for i = 1:m</span><br><span class="line"></span><br><span class="line">  for j = 1:m</span><br><span class="line"></span><br><span class="line">    model = svmTrain(X, y, C_trial(i), @(x1, x2) gaussianKernel(x1, x2, sigma_trial(j)));</span><br><span class="line">    predictions = svmPredict(model, Xval);</span><br><span class="line"></span><br><span class="line">    error = mean(double(predictions ~= yval));</span><br><span class="line"></span><br><span class="line">    if error &lt; initial_error</span><br><span class="line"></span><br><span class="line">      initial_error = error;</span><br><span class="line">      C_temp = C_trial(i);</span><br><span class="line">      sigma_temp = sigma_trial(j);</span><br><span class="line"></span><br><span class="line">    endif</span><br><span class="line"></span><br><span class="line">  endfor</span><br><span class="line"></span><br><span class="line">endfor</span><br><span class="line"></span><br><span class="line">C = C_temp;</span><br><span class="line">sigma = sigma_temp;</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<ul>
<li>在 Octave 上選擇 <code>C</code> 與 $σ^2$ 的函式</li>
</ul>
<h3 id="使用支持向量機-SVM"><a href="#使用支持向量機-SVM" class="headerlink" title="使用支持向量機 ( SVM )"></a>使用支持向量機 ( SVM )</h3><ul>
<li>使用 SVM 的軟件包 ( EX : liblinear、libsvm、… ) 來解出 θ</li>
<li><p>使用支持向量機需要做的事 :</p>
<ul>
<li>選擇參數 <code>C</code></li>
<li><p>選擇核函數</p>
<ul>
<li><p>沒有使用核函數 ( 又稱線性核函數 )</p>
<ul>
<li>適合特徵量 ( n ) 大，訓練資料量 ( m ) 小</li>
<li>θ<sup>T</sup>x ≥ 0，預測 y = 1</li>
</ul>
</li>
<li><p>使用高斯核函數</p>
<ul>
<li>適合特徵量 ( n ) 小，訓練資料量 ( m ) 大</li>
<li>θ<sup>T</sup>f ≥ 0，預測 y = 1</li>
<li>需要選擇參數 $σ^2$</li>
<li>需要提供所使用的核函數</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="高斯核函數-Gaussian-Kernel"><a href="#高斯核函數-Gaussian-Kernel" class="headerlink" title="高斯核函數 ( Gaussian Kernel )"></a>高斯核函數 ( Gaussian Kernel )</h4><p><img src="https://i.imgur.com/tbBFJbM.png" alt></p>
<ul>
<li>f 為 f<sup>(i)</sup>，x1 為 x<sup>(i)</sup>，x2 為 l<sup>(j)</sup></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">function sim = gaussianKernel(x1, x2, sigma)</span><br><span class="line"></span><br><span class="line">x1 = x1(:);</span><br><span class="line">x2 = x2(:);</span><br><span class="line">sim = 0;</span><br><span class="line"></span><br><span class="line">margin = sum((x1 - x2) .^ 2);</span><br><span class="line">sigma2 = 2 * (sigma ^ 2);</span><br><span class="line">sim = exp(- margin / sigma2);</span><br><span class="line">    </span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<ul>
<li>在 Octave 上的高斯核函數函式</li>
</ul>
<p>在使用高斯核函數前<strong>應該先進行特徵縮放 ( Feature Scaling )</strong></p>
<ul>
<li>這樣才能保證 SVM 會同等的關注到所有不同的特徵量</li>
</ul>
<h4 id="其他的核函數"><a href="#其他的核函數" class="headerlink" title="其他的核函數"></a>其他的核函數</h4><p>不管甚麼核函數都必須滿足默塞爾定理 (Mercer’s Theorem)</p>
<ul>
<li><p>多項式核函數 ( Polynomial Kernel )</p>
<ul>
<li>k(x, l) = (x<sup>T</sup>l + constant)<sup>degree</sup></li>
</ul>
</li>
<li><p>字串核函數 ( String Kernel ) : 資料為字符串時有時會用到</p>
</li>
<li>卡方核函數 ( chi-square Kernel )</li>
<li>直方圖交叉核函數 ( histogram intersection Kernel )</li>
</ul>
<h4 id="多類別分類"><a href="#多類別分類" class="headerlink" title="多類別分類"></a>多類別分類</h4><ul>
<li>許多 SVM 的軟件包已經有內建多類別分類</li>
<li><p>如果沒有 :</p>
<ul>
<li>使用之前在邏輯回歸提過的 One-vs-all 方法</li>
<li>訓練 K 個 SVM 去分類 K 個類別，得到 θ<sup>(1)</sup>、θ<sup>(2)</sup>、…、θ<sup>(K)</sup></li>
<li>將新數值帶入每個 SVM，取輸出值最大的 SVM ( (θ<sup>(i)</sup>)<sup>T</sup>x )</li>
</ul>
</li>
</ul>
<h4 id="邏輯回歸-vs-SVMs"><a href="#邏輯回歸-vs-SVMs" class="headerlink" title="邏輯回歸 vs SVMs"></a>邏輯回歸 vs SVMs</h4><ul>
<li><code>n</code> : 特徵量</li>
<li><p><code>m</code> : 訓練資料量</p>
</li>
<li><p>如果 n 很大 ( n ≥ m ) :</p>
<ul>
<li>使用邏輯回歸或沒有 kernel ( 線性核函數 ) 的 SVM</li>
</ul>
</li>
<li><p>如果 n 很小，m 適中 ( n = 1 ~ 1000，m = 10 ~ 10000 ) :</p>
<ul>
<li>使用有高斯核函數的 SVM</li>
</ul>
</li>
<li><p>如果 n 很小，m 很大 ( n = 1 ~ 1000，m = 50000+ ) :</p>
<ul>
<li>使用有高斯核函數的 SVM 會很緩慢</li>
<li>增加特徵量後，使用邏輯回歸或沒有 kernel ( 線性核函數 ) 的 SVM</li>
</ul>
</li>
<li><p>神經網路在這些條件下都能有很好的表現，但訓練速度相對緩慢</p>
</li>
</ul>
<h6 id="tags-筆記-機器學習-支持向量機SVM"><a href="#tags-筆記-機器學習-支持向量機SVM" class="headerlink" title="tags: 筆記 機器學習 支持向量機SVM"></a>tags: <code>筆記</code> <code>機器學習</code> <code>支持向量機SVM</code></h6></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Kenny Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://kennyliblog.nctu.me/2019/08/15/Machine-learning-Support-Vector-Machines/">https://kennyliblog.nctu.me/2019/08/15/Machine-learning-Support-Vector-Machines/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/筆記/">筆記    </a><a class="post-meta__tags" href="/tags/機器學習/">機器學習    </a><a class="post-meta__tags" href="/tags/支持向量機SVM/">支持向量機SVM    </a></div><div class="post_share"><div class="social-share" data-image="/img/note.jpg" data-sites="facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2019/08/13/Hexo-use-mathJax/"><img class="next_cover lozad" data-src="/img/problem.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>問題 - 如何在 Hexo 上使用數學式</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/07/24/Machine-learning-overfitting-problem/" title="筆記 - 機器學習 過度擬合問題 ( overfitting )"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 過度擬合問題 ( overfitting )</div></a></div><div class="relatedPosts_item"><a href="/2019/07/12/Octave-basic/" title="筆記 - Octave 基礎"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - Octave 基礎</div></a></div><div class="relatedPosts_item"><a href="/2019/07/19/Machine-learning-logistic-regression/" title="筆記 - 機器學習 邏輯回歸"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 邏輯回歸</div></a></div><div class="relatedPosts_item"><a href="/2019/07/12/Machine-learning-basic-linear-regression/" title="筆記 - 機器學習 基礎與線性回歸"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 基礎與線性回歸</div></a></div><div class="relatedPosts_item"><a href="/2019/07/26/Machine-learning-neural-network-representation/" title="筆記 - 機器學習 神經網路 ( Neural Network ) - 表層結構"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 神經網路 ( Neural Network ) - 表層結構</div></a></div><div class="relatedPosts_item"><a href="/2019/08/09/Machine-learning-system-design/" title="筆記 - 機器學習 系統的設計"><img class="relatedPosts_cover lozad" data-src="/img/note.jpg"><div class="relatedPosts_title">筆記 - 機器學習 系統的設計</div></a></div></div><div class="clear_both"></div></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 By Kenny Li</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="Read Mode"> </i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion">繁</a><i class="fa fa-moon-o nightshift" id="nightshift" title="Dark Mode"></i></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/baidupush.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/nightshift.js"></script><script id="ribbon" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/js/piao.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script></body></html>